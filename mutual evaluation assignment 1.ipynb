{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3.1 数据摘要和可视化\n",
    "    - 数据摘要\n",
    "        - 标称属性，给出每个可能取值的频数\n",
    "        - 数值属性，给出5数概括及缺失值的个数\n",
    "    - 数据可视化\n",
    "        - 使用直方图、盒图等检查数据分布及离群点\n",
    "- 3.2 数据缺失的处理\n",
    "    - 观察数据集中缺失数据，分析其缺失的原因。分别使用下列四种策略对缺失值进行处理:\n",
    "        - 将缺失部分剔除\n",
    "        - 用最高频率值来填补缺失值\n",
    "        - 通过属性的相关关系来填补缺失值\n",
    "        - 通过数据对象之间的相似性来填补缺失值\n",
    "    - 注意：在处理后完成，要对比新旧数据集的差异。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from impyute.imputation.cs import fast_knn\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', None)  # 设置行不限制数量\n",
    "pd.set_option('display.max_columns', None)  # 设置列不限制数量\n",
    "pd.set_option('max_colwidth', 100)  # 设置value的显示长度为100，默认为50\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['KaiTi']\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "数据摘要\n",
    "    标称属性，给出每个可能取值的频数\n",
    "    数值属性，给出5数概括及缺失值的个数\n",
    "'''\n",
    "\n",
    "\n",
    "def nominal_summary(df: pd.DataFrame, nominal_index, new_df=None, n=50):\n",
    "    '''标称属性 给出每个可能取值的频数 可视化直方图'''\n",
    "    for key in nominal_index:\n",
    "        data = df[key].value_counts()\n",
    "        if new_df is None:\n",
    "            plt.figure(figsize=(11, 6))\n",
    "            plt.title(data.name, fontsize=30)\n",
    "            plt.bar(data.index[:n], data.values[:n], width=0.8)\n",
    "            plt.xticks(rotation=90)\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.figure(figsize=(22, 6))\n",
    "            plt.subplot(121)\n",
    "            plt.title(data.name, fontsize=30)\n",
    "            plt.bar(data.index[:n], data.values[:n], width=0.8, label=data.name)\n",
    "            plt.xticks(rotation=90)\n",
    "            plt.subplot(122)\n",
    "            new_data = new_df[key].value_counts()\n",
    "            plt.title(new_data.name, fontsize=30)\n",
    "            plt.bar(new_data.index[:n], new_data.values[:n], width=0.8, label='processed '+new_data.name)\n",
    "            plt.xticks(rotation=90)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "def numerical_summary(df: pd.DataFrame, numerical_index, new_df=None):\n",
    "    '''数值属性 给出五数概括及缺失值个数'''\n",
    "    for key in numerical_index:\n",
    "        shape = df.shape\n",
    "        data = df[key]\n",
    "        info = data.describe()\n",
    "        print('descriptive statistics ({}):'.format(data.name))\n",
    "        # 五数概括，直接利用函数计算  缺失值为总行数-有值得行数\n",
    "        print(\"          Data: Min: {:< 2.4f}\\tQ1(25%): {:< 2.4f}\\tQ2(50%): {:< 2.4f}\\tQ3(75%): {:< 2.4f}\\tMax: {:< 2.4f}\\tMissing: {:d}\".format(\n",
    "            info['min'], info['25%'], info['50%'], info['75%'], info['max'], int(shape[0] - info['count'])\n",
    "        ))\n",
    "        if new_df is not None:\n",
    "            new_shape = new_df.shape\n",
    "            new_data = new_df[key]\n",
    "            new_info = new_data.describe()\n",
    "            print(\"Processed Data: Min: {:< 2.4f}\\tQ1(25%): {:< 2.4f}\\tQ2(50%): {:< 2.4f}\\tQ3(75%): {:< 2.4f}\\tMax: {:< 2.4f}\\tMissing: {:d}\".format(\n",
    "                new_info['min'], new_info['25%'], new_info['50%'], new_info['75%'], new_info['max'], int(new_shape[0] - new_info['count'])\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "数据可视化\n",
    "    使用直方图、盒图等检查数据分布及离群点\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def box_plot(df, label=None, new_df=None):\n",
    "    if new_df is None:\n",
    "        plt.figure()\n",
    "        plt.title('Data')\n",
    "        sns.boxplot(y=label, data=df, palette='Set2')\n",
    "    else:\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        plt.subplot(121)\n",
    "        plt.title('Data')\n",
    "        sns.boxplot(y=label, data=df, palette='Set1')\n",
    "        plt.subplot(122)\n",
    "        plt.title('Processed Data')\n",
    "        sns.boxplot(y=label, data=new_df, palette='Set2')\n",
    "\n",
    "\n",
    "def hist_plot(df, label=None, bins=10, new_df=None):\n",
    "    if new_df is None:\n",
    "        plt.figure()\n",
    "        plt.title('Data')\n",
    "        sns.histplot(df[label].dropna(), bins=bins, kde=False)\n",
    "    else:\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        plt.subplot(121)\n",
    "        plt.title('Data')\n",
    "        sns.histplot(df[label].dropna(), bins=bins, kde=False)\n",
    "        plt.subplot(122)\n",
    "        plt.title('Processed Data')\n",
    "        sns.histplot(new_df[label].dropna(), bins=bins, kde=False)\n",
    "\n",
    "\n",
    "def viz_pairs(df, labels=None, bins=20, new_df=None):\n",
    "    for label in labels:\n",
    "        box_plot(df, label=label, new_df=new_df)\n",
    "        hist_plot(df, label=label, bins=bins, new_df=new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "数据缺失值处理\n",
    "    将缺失部分剔除\n",
    "    用最高频率值来填补缺失值\n",
    "    通过属性的相关关系来填补缺失值\n",
    "    通过数据对象之间的相似性来填补缺失值\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def remove_missing_parts(df: pd.DataFrame):\n",
    "    '''将缺失部分剔除'''\n",
    "    return df.dropna(axis=0)\n",
    "\n",
    "\n",
    "def fill_in_missing_values_with_the_highest_frequency_value(df: pd.DataFrame):\n",
    "    '''用最高频率值来填补缺失值'''\n",
    "    cpdf = df.copy(deep=True)\n",
    "    for key in cpdf.columns:\n",
    "        cpdf[key].fillna(cpdf[key].mode()[0], inplace=True)\n",
    "    return cpdf\n",
    "\n",
    "\n",
    "def fill_in_missing_values_through_attribute_correlation(df: pd.DataFrame, miss_index, complete_index):\n",
    "    '''通过属性的相关关系来填补缺失值'''\n",
    "    \n",
    "    def set_miss_values(df: pd.DataFrame, complete_index):\n",
    "        enc_label = OrdinalEncoder()\n",
    "        enc_feature = OrdinalEncoder()\n",
    "        missing_index = complete_index[0]\n",
    "        train_df = df[complete_index]\n",
    "\n",
    "        known_values = np.array(train_df[train_df[missing_index].notnull()])\n",
    "        unknow_values = np.array(train_df[train_df[missing_index].isnull()])\n",
    " \n",
    "        y = known_values[:, 0].reshape(-1, 1)\n",
    "        enc_label.fit(y)\n",
    "        y = enc_label.transform(y)\n",
    "\n",
    "        X = known_values[:, 1:]\n",
    "        test_X = unknow_values[:, 1:]\n",
    "        all_X = np.row_stack((X, test_X))\n",
    "        enc_feature.fit(all_X)\n",
    "        X = enc_feature.transform(X)\n",
    "\n",
    "        rfr = RandomForestRegressor(random_state=0, n_estimators=20, n_jobs=10)\n",
    "        rfr.fit(X, y.ravel())\n",
    "\n",
    "        predicted_values = rfr.predict(enc_feature.transform(unknow_values[:, 1:]))\n",
    "        predicted_values = enc_label.inverse_transform(predicted_values.reshape(-1, 1))\n",
    "\n",
    "        df.loc[(df[missing_index].isnull()), missing_index] = predicted_values\n",
    "        return df\n",
    "    \n",
    "    cpdf = df.copy(deep=True)\n",
    "    for i in range(0, len(miss_index)):\n",
    "        complete_index.insert(0, miss_index[i])\n",
    "        cpdf = set_miss_values(cpdf, complete_index)\n",
    "    return cpdf\n",
    "\n",
    "\n",
    "def fill_in_missing_values_through_similarity_between_data_objects(df: pd.DataFrame, numerical_index, k=30):\n",
    "    '''通过数据对象之间的相似性来填补缺失值'''\n",
    "    cpdf = df.copy(deep=True)\n",
    "    imputed_training = fast_knn(cpdf[numerical_index].values, k=k)\n",
    "    imputed_training = pd.DataFrame(data=imputed_training, columns=numerical_index)\n",
    "    cpdf[numerical_index] = imputed_training[numerical_index]\n",
    "    return cpdf\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Alzheimer Disease and Healthy Aging Data in US](https://www.kaggle.com/datasets/ananthu19/alzheimer-disease-and-healthy-aging-data-in-us)\n",
    "\n",
    "- The Alzheimer's Disease and Healthy Aging Data in the US dataset is a comprehensive collection of data on the health and well-being of older Americans. The dataset includes information on a wide range of variables, such as demographic characteristics, health conditions, healthcare utilization, and health behaviors.\n",
    "\n",
    "- The dataset was compiled by the Centers for Disease Control and Prevention (CDC) and includes data from several national surveys, including the Behavioral Risk Factor Surveillance System (BRFSS), the National Health and Nutrition Examination Survey (NHANES), and the National Health Interview Survey (NHIS).\n",
    "\n",
    "- The primary focus of the dataset is on Alzheimer's disease and related dementias, including prevalence, incidence, risk factors, and outcomes.The data can be used to identify trends and patterns in the prevalence and incidence of these conditions, as well as to explore potential risk factors and interventions that may help to prevent or mitigate their impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Alzheimer Disease and Healthy Aging Data In US.csv'\n",
    "data = pd.read_csv(path, sep=',')\n",
    "# Sample_Size全是空，因此直接将其删掉\n",
    "data = data.drop(columns=[\"Sample_Size\"])\n",
    "# 处理异常值\n",
    "data['Low_Confidence_Limit'] = data['Low_Confidence_Limit'].apply(lambda x: None if x == '.' else float(x))\n",
    "data['High_Confidence_Limit'] = data['High_Confidence_Limit'].apply(lambda x: None if x == '.' else float(x))\n",
    "data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据摘要和可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标称属性\n",
    "nominal_index = ['LocationAbbr', 'LocationDesc', 'Datasource', 'Class', 'Topic', 'Question', 'Data_Value_Unit', 'DataValueTypeID', 'Data_Value_Type', 'StratificationCategory1', 'Stratification1', 'StratificationCategory2', 'Stratification2', 'Geolocation', 'ClassID', 'TopicID', 'QuestionID', 'StratificationCategoryID1', 'StratificationID1', 'StratificationCategoryID2', 'StratificationID2']\n",
    "# 数值属性\n",
    "numerical_index = ['YearStart', 'YearEnd', 'Data_Value', 'Data_Value_Alt', 'Low_Confidence_Limit', 'High_Confidence_Limit', 'LocationID']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标称属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_summary(data, nominal_index=nominal_index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数值属性\n",
    "\n",
    "- Data_Value, Data_Value_Alt, Low_Confidence_Limit, High_Confidence_Limit属性都有很多的缺失\n",
    "- LocationID虽然没有缺失的数据点，但是明显存在离群点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_summary(data, numerical_index=numerical_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据可视化：使用直方图、盒图等检查数据分布及离群点\n",
    "viz_pairs(data, labels=numerical_index, bins=20, new_df=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据缺失的处理"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将缺失部分剔除\n",
    "\n",
    "- 凡是带有缺失属性的数据一律删除\n",
    "- 虽然数据量大幅缩小，但是确实去除了一些噪点，比如LocationID中的离群点消失了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = remove_missing_parts(data)\n",
    "new_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_summary(data, nominal_index, new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_summary(data, numerical_index=numerical_index, new_df=new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_pairs(data, labels=numerical_index, bins=20, new_df=new_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用最高频率值来填补缺失值\n",
    "\n",
    "- 简单粗暴，将所有缺失值用其所在的数据中出现次数最多的数值进行填充，可以将数据恢复满\n",
    "- 在某些情况下不符合实际情况，可能会改变原有的数据分布，并且离群点不会消失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = fill_in_missing_values_with_the_highest_frequency_value(data)\n",
    "new_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_summary(data, nominal_index, new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_summary(data, numerical_index=numerical_index, new_df=new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_pairs(data, labels=numerical_index, bins=20, new_df=new_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过属性的相关关系来填补缺失值\n",
    "\n",
    "- 训练一个模型，根据非缺失值来预测缺失值，如果能学到潜在的属性相关关系将会非常有效\n",
    "- 花费时间较长，且无法保证训练出的模型预测值一定准确，离群点不会消失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_index = [k for k, v in data.isna().sum().items() if v > 0]\n",
    "comp_index = [k for k, v in data.isna().sum().items() if v == 0]\n",
    "new_data = fill_in_missing_values_through_attribute_correlation(data, miss_index=miss_index, complete_index=comp_index)\n",
    "new_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_summary(data, nominal_index, new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_summary(data, numerical_index=numerical_index, new_df=new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_pairs(data, labels=numerical_index, bins=20, new_df=new_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过数据对象之间的相似性来填补缺失值\n",
    "\n",
    "- 相似性即距离的度量，使用k近邻策略修补缺失值\n",
    "- 由于距离度量计算的原因，部分缺失值可能找不到与其相似的值来填补"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = fill_in_missing_values_through_similarity_between_data_objects(data, numerical_index=numerical_index, k=100)\n",
    "new_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_summary(data, nominal_index, new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_summary(data, numerical_index=numerical_index, new_df=new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_pairs(data, labels=numerical_index, bins=20, new_df=new_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Movies Dataset from Pirated Sites](https://www.kaggle.com/datasets/arsalanrehman/movies-dataset-from-piracy-website)\n",
    "\n",
    "- This dataset has been gathered from a pirated website that has a user base of around 2M visitors per month. This data contains more than 20,000+ movies from all industries such as Hollywood, Bollywood, Anime, etc.\n",
    "- Data Fields\n",
    "    - id: movie's unique id\n",
    "    - title: movie's name\n",
    "    - storyline: a short description of the movie\n",
    "    - views: no. of clicks per movie\n",
    "    - downloads: no. of downloads per movie\n",
    "    - IMDb-rating: rating\n",
    "    - appropriate_for: R-rated, PG-13, etc\n",
    "    - language: this can be multiple languages also\n",
    "    - industry: Hollywood, Bollywood, etc.\n",
    "    - posted_date: when the movie is posted on the platform\n",
    "    - release_date: when the movie is released worldwide\n",
    "    - runtime: in minutes\n",
    "    - director: director's name\n",
    "    - writer: list of all the writers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'movies_dataset.csv'\n",
    "data = pd.read_csv(path, sep=',')\n",
    "# Unnamed: 0无意义，因此直接将其删掉\n",
    "data = data.drop(columns=[\"Unnamed: 0\"])\n",
    "data['downloads'] = data['downloads'].apply(lambda x: float(x.replace(',', '')) if isinstance(x, str) else x)\n",
    "data['views'] = data['views'].apply(lambda x: float(x.replace(',', '')) if isinstance(x, str) else x)\n",
    "# data['posted_date'] = pd.to_datetime(data['posted_date'])\n",
    "# data['release_date'] = pd.to_datetime(data['release_date'])\n",
    "def f(x):\n",
    "    if x == x:\n",
    "        y = x.replace(' ', '').replace('min', '').replace('m', '').split('h')\n",
    "        t = [60, 1]\n",
    "        ans = 0\n",
    "        for i in range(len(y)):\n",
    "            if len(y[len(y) - 1 - i]) > 0:\n",
    "                ans += t[1 - i] * int(y[len(y) - 1 - i])\n",
    "        return ans\n",
    "    else:\n",
    "        return x\n",
    "data['run_time'] = data['run_time'].apply(f)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据摘要和可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标称属性\n",
    "nominal_index = ['appropriate_for', 'director', 'industry', 'language', 'posted_date', 'release_date', 'storyline', 'title', 'writer']\n",
    "# 数值属性\n",
    "numerical_index = ['IMDb-rating', 'id', 'downloads', 'run_time', 'views']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标称属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_summary(data, nominal_index=nominal_index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数值属性\n",
    "\n",
    "- 数据量虽然少，但是不少属性都存在大量离群点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_summary(data, numerical_index=numerical_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据可视化：使用直方图、盒图等检查数据分布及离群点\n",
    "viz_pairs(data, labels=numerical_index, bins=20, new_df=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据缺失的处理\n",
    "\n",
    "- 实现与细节的讨论与上一数据集相似"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将缺失部分剔除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = remove_missing_parts(data)\n",
    "new_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_summary(data, nominal_index, new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_summary(data, numerical_index=numerical_index, new_df=new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_pairs(data, labels=numerical_index, bins=20, new_df=new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用最高频率值来填补缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = fill_in_missing_values_with_the_highest_frequency_value(data)\n",
    "new_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_summary(data, nominal_index, new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_summary(data, numerical_index=numerical_index, new_df=new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_pairs(data, labels=numerical_index, bins=20, new_df=new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过属性的相关关系来填补缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_index = [k for k, v in data.isna().sum().items() if v > 0]\n",
    "comp_index = [k for k, v in data.isna().sum().items() if v == 0]\n",
    "new_data = fill_in_missing_values_through_attribute_correlation(data, miss_index=miss_index, complete_index=comp_index)\n",
    "new_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_summary(data, nominal_index, new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_summary(data, numerical_index=numerical_index, new_df=new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_pairs(data, labels=numerical_index, bins=20, new_df=new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过数据对象之间的相似性来填补缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = fill_in_missing_values_through_similarity_between_data_objects(data, numerical_index=numerical_index, k=10)\n",
    "new_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_summary(data, nominal_index, new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_summary(data, numerical_index=numerical_index, new_df=new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_pairs(data, labels=numerical_index, bins=20, new_df=new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
